import pytest\nimport subprocess\nimport os\nimport json\nimport glob\nimport shutil\nimport sys\n\n# Add project root to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\n# Location of the script to test\SCRIPT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \'../src/json_splitter.py\'))\n# Location of test data\nTEST_DATA_DIR = os.path.join(os.path.dirname(__file__), \'data\')\nSAMPLE_ARRAY_FILE = os.path.join(TEST_DATA_DIR, \'sample_array.json\')\nLARGE_JSON_FILE = os.path.join(TEST_DATA_DIR, \'json-40mb.json\')\n\n@pytest.fixture\ndef temp_output_dir(tmp_path):\n    \"\"\"Create a temporary directory for test output.\"\"\"\n    output_dir = tmp_path / \"output\"\n    output_dir.mkdir()\n    yield output_dir\n    # tmp_path fixture handles cleanup automatically\n\ndef run_splitter(args):\n    \"\"\"Helper function to run the splitter script as a subprocess.\"\"\"\n    cmd = [sys.executable, SCRIPT_PATH] + args\n    # Use repr() for cleaner command logging, especially with spaces/quotes\n    print(f\"\\nRunning command: {repr(cmd)}\")\n    # Ensure consistent encoding and capture output\n    result = subprocess.run(cmd, capture_output=True, text=True, check=False, encoding=\'utf-8\')\n    print(f\"STDOUT:\\n{result.stdout}\")\n    print(f\"STDERR:\\n{result.stderr}\")\n    # Raise exception if script returned non-zero exit code for easier debugging\n    result.check_returncode()\n    return result\n\ndef load_json_output(filepath):\n    \"\"\"Load JSON from a file, failing the test on error.\"\"\"\n    try:\n        with open(filepath, \'r\', encoding=\'utf-8\') as f:\n            return json.load(f)\n    except json.JSONDecodeError as e:\n        pytest.fail(f\"Failed to decode JSON from {filepath}: {e}\")\n    except FileNotFoundError:\n        pytest.fail(f\"Output file not found: {filepath}\")\n    except Exception as e:\n        pytest.fail(f\"Unexpected error loading JSON from {filepath}: {e}\")\n\ndef load_jsonl_output(filepath):\n    \"\"\"Load list of objects from a JSON Lines file, failing the test on error.\"\"\"\n    data = []\n    try:\n        with open(filepath, \'r\', encoding=\'utf-8\') as f:\n            for line_num, line in enumerate(f, 1):\n                line = line.strip()\n                if line:\n                    try:\n                        data.append(json.loads(line))\n                    except json.JSONDecodeError as e:\n                        pytest.fail(f\"Failed to decode JSONL line {line_num} in {filepath}: {e}\\nLine content: {line!r}\")\n        return data\n    except FileNotFoundError:\n        pytest.fail(f\"Output file not found: {filepath}\")\n    except Exception as e:\n         pytest.fail(f\"Unexpected error loading JSONL from {filepath}: {e}\")\n\n# --- Integration Tests --- #\n\ndef test_split_by_count_basic(temp_output_dir):\n    \"\"\"Test basic splitting by count into JSON array files.\"\"\"\n    output_prefix = str(temp_output_dir / \"count_basic\")\n    run_splitter([\n        SAMPLE_ARRAY_FILE,\n        output_prefix,\n        \"--split-by\", \"count\",\n        \"--value\", \"3\",\n        \"--path\", \"item\"\n    ])\n\n    # Check files created (using the updated naming convention)\n    files = sorted(glob.glob(f\"{output_prefix}_chunk_*.json\"))\n    assert len(files) == 3, f\"Expected 3 files, found {len(files)}: {files}\"\n\n    # Check content\n    data0 = load_json_output(files[0])\n    data1 = load_json_output(files[1])\n    data2 = load_json_output(files[2])\n\n    assert len(data0) == 3\n    assert data0[0][\"id\"] == 1\n    assert data0[2][\"id\"] == 3\n\n    assert len(data1) == 3\n    assert data1[0][\"id\"] == 4\n    assert data1[2][\"id\"] == 6\n\n    assert len(data2) == 1\n    assert data2[0][\"id\"] == 7\n\ndef test_split_by_count_jsonl(temp_output_dir):\n    \"\"\"Test splitting by count into JSONL files.\"\"\"\n    output_prefix = str(temp_output_dir / \"count_jsonl\")\n    run_splitter([\n        SAMPLE_ARRAY_FILE,\n        output_prefix,\n        \"--split-by\", \"count\",\n        \"--value\", \"2\",\n        \"--path\", \"item\",\n        \"--output-format\", \"jsonl\"\n    ])\n\n    files = sorted(glob.glob(f\"{output_prefix}_chunk_*.jsonl\"))\n    assert len(files) == 4, f\"Expected 4 files, found {len(files)}: {files}\" # 7 items, chunks of 2 -> 4 files\n\n    # Check content of first and last\n    data0 = load_jsonl_output(files[0])\n    assert len(data0) == 2\n    assert data0[0][\"id\"] == 1\n    assert data0[1][\"id\"] == 2\n\n    data3 = load_jsonl_output(files[3])\n    assert len(data3) == 1\n    assert data3[0][\"id\"] == 7\n\ndef test_split_by_size_basic(temp_output_dir):\n    \"\"\"Test splitting by size into JSON array files using a larger file.\"\"\"\n    output_prefix = str(temp_output_dir / \"size_basic\")\n    split_size_mb = 10\n    split_size_bytes = split_size_mb * 1024 * 1024\n    # Rough expectation: 40MB file split into 10MB chunks -> 4 files\n    # Allow some tolerance due to approximate splitting\n    expected_min_files = 3\n    expected_max_files = 5\n\n    run_splitter([\n        LARGE_JSON_FILE,\n        output_prefix,\n        \"--split-by\", \"size\",\n        \"--value\", f\"{split_size_mb}MB\",\n        \"--path\", \"item\" # Assuming the large file is also an array at the root\"\n    ])\n\n    files = sorted(glob.glob(f\"{output_prefix}_chunk_*.json\"))\n    assert expected_min_files <= len(files) <= expected_max_files, \
        f"Expected {expected_min_files}-{expected_max_files} files for ~{split_size_mb}MB split, found {len(files)}"

    total_size = 0
    for i, f_path in enumerate(files):\n        # Check file is valid JSON\n        load_json_output(f_path)\n        # Check file size (approximate)\n        size = os.path.getsize(f_path)\n        total_size += size\n        print(f\"  File {os.path.basename(f_path)} size: {size / (1024*1024):.2f} MB\")\n        # Allow for some variation, especially the last file\n        # Increase tolerance factor (e.g., 50%)\n        if i < len(files) - 1: # Don't check last file size too strictly\n             assert size < split_size_bytes * 1.5, f\"File {f_path} size {size} significantly exceeds target {split_size_bytes}\"\n\n    # Check total size is roughly the original size (within reason, formatting might change things)\n    original_size = os.path.getsize(LARGE_JSON_FILE)\n    assert original_size * 0.9 < total_size < original_size * 1.1, \
        f"Total output size {total_size} differs significantly from original {original_size}"
\ndef test_split_by_size_jsonl(temp_output_dir):\n    \"\"\"Test splitting by size into JSONL files using a larger file.\"\"\"\n    output_prefix = str(temp_output_dir / \"size_jsonl\")\n    split_size_mb = 8 # Use a slightly different size\n    split_size_bytes = split_size_mb * 1024 * 1024\n    # Rough expectation: 40MB file / 8MB chunks -> 5 files\n    expected_min_files = 4\n    expected_max_files = 6\n\n    run_splitter([\n        LARGE_JSON_FILE,\n        output_prefix,\n        \"--split-by\", \"size\",\n        \"--value\", f\"{split_size_mb}MB\",\n        \"--path\", \"item\", # Assuming the large file is also an array at the root\"\n        \"--output-format\", \"jsonl\"\n    ])\n\n    files = sorted(glob.glob(f\"{output_prefix}_chunk_*.jsonl\"))\n    assert expected_min_files <= len(files) <= expected_max_files, (
        f"Expected {expected_min_files}-{expected_max_files} files for ~{split_size_mb}MB split, found {len(files)}"
    )

    total_size = 0
    for i, f_path in enumerate(files):\n        # Check file is valid JSONL\n        load_jsonl_output(f_path)\n        # Check file size (approximate)\n        size = os.path.getsize(f_path)\n        total_size += size\n        print(f\"  File {os.path.basename(f_path)} size: {size / (1024*1024):.2f} MB\")\n        # JSONL size calculation is more direct, tolerance can be smaller? Maybe 1.3x\n        if i < len(files) - 1: # Don't check last file size too strictly\n            assert size < split_size_bytes * 1.3, f\"File {f_path} size {size} significantly exceeds target {split_size_bytes}\"\n\n    # Check total size is roughly the original size (JSONL might be slightly smaller than array JSON)\n    original_size = os.path.getsize(LARGE_JSON_FILE)\n    assert original_size * 0.85 < total_size < original_size * 1.05, (
         f"Total output size {total_size} differs significantly from original {original_size}"
    )
\n\n# TODO: Add test_split_by_key_basic\n# TODO: Add test_split_by_key_missing_group (default)\n# TODO: Add test_split_by_key_missing_skip\n# TODO: Add test_split_by_key_missing_error\n# TODO: Add test_split_by_key_invalid_item_warn (default)\n# TODO: Add test_split_by_key_invalid_item_skip\n# TODO: Add test_split_by_key_invalid_item_error\n# TODO: Add test_split_count_with_max_records\n# TODO: Add test_split_count_with_max_size\n# TODO: Add test_split_key_with_max_records\n# TODO: Add test_split_key_with_max_size\n# TODO: Add test_error_file_not_found\n# TODO: Add test_error_invalid_json\n# TODO: Add test_error_invalid_args (e.g., negative count, bad size format)\n``` 